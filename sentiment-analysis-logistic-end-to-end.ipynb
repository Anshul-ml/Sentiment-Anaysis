{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\n\n# Record start time\nstart_time = time.time()\n\n\nimport nltk\nfrom os import getcwd\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import twitter_samples\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer#reducing words to their base form.\nfrom nltk.tokenize import TweetTokenizer\nimport re\nimport string\n\n\nnltk.download('stopwords')\nnltk.download('twitter_samples')\n\n\nend_time = time.time()\n\n# Calculate total time\ntotal_time = end_time - start_time\n\n# Print the total time\nprint(f\"Total time taken: {total_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-11-27T08:39:31.454468Z","iopub.execute_input":"2023-11-27T08:39:31.455528Z","iopub.status.idle":"2023-11-27T08:39:31.463828Z","shell.execute_reply.started":"2023-11-27T08:39:31.455475Z","shell.execute_reply":"2023-11-27T08:39:31.462791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Total time taken: 0.0005914138793945312 seconds","metadata":{}},{"cell_type":"code","source":"import time\n\n# Record start time\nstart_time = time.time()\n\n\n\n\n\n\n#FUNTIONS:\ndef process_tweet(tweet):\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    tweet = re.sub(r'\\$\\w*|^RT[\\s]+|https?://[^\\s\\n\\r]+|#', '', tweet)\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n    tweets_clean = []\n    for word in tweet_tokens:\n        if(word not in stopwords_english and word not in string.punctuation):\n            stem_word = stemmer.stem(word)\n            tweets_clean.append(stem_word)\n    return tweets_clean\n    \ndef build_freqs(tweets, ys):\n    ys_list = np.squeeze(ys).tolist()\n    freqs= {}\n    for y,tweet in zip(ys_list, tweets):\n        for word in process_tweet(tweet):\n            pair = (word,y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n    return freqs\n\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\ndef gradientDescent(x,y,theta,alpha,num_iteraters):\n    m = x.shape[0]\n    for i in range(0, num_iteraters):\n        z = np.dot(x, theta) \n        h = sigmoid(z)\n        J = -1/m * (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))\n        theta = theta - alpha/m * np.dot(x.T, (h-y))\n    J = float(J)\n    return J, theta\n\ndef extract_feature(tweet, freqs, process_tweet=process_tweet):\n    word_l = process_tweet(tweet)\n    x = np.zeros(3)\n    x[0] = 1\n    for word in word_l:\n        x[1] += freqs.get((word,1),0)\n        x[2] += freqs.get((word,0),0)\n    # the code ensures that x has a shape of (1, 3) after adding a batch dimension.\n    x = x[None, :]  # adding batch dimension for further processing\n    assert (x.shape == (1, 3))\n    return x\n    \ndef predict_tweet(tweet, freqs, theta):\n    x = extract_feature(tweet, freqs)\n    y_pred = sigmoid(np.dot(x,theta))\n    return y_pred\n\n#data assign\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\ntrain_x = train_pos + train_neg \ntest_x = test_pos + test_neg\ntrain_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n\nfreqs = build_freqs(train_x,train_y)\n\n#Training\nX = np.zeros((len(train_x),3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_feature(train_x[i], freqs)\n\nY = train_y\nJ, theta = gradientDescent(X,Y,np.zeros((3,1)),1e-9,1500)\n\n#prediction\nmy_tweet = input(\"Enter a tweet: \")\nprint(process_tweet(my_tweet))\ny_hat = predict_tweet(my_tweet, freqs, theta)\nprint(y_hat)\nif y_hat > 0.5099:\n    print('Positive sentiment')\nelif y_hat<=0.5099 and y_hat>=0.4955:\n    print('Neutral Sentiment')\nelse: \n    print('Negative sentiment')\n\n    \n    \nend_time = time.time()\n\n# Calculate total time\ntotal_time = end_time - start_time\n\n# Print the total time\nprint(f\"Total time taken: {total_time} seconds\")","metadata":{"_uuid":"0974895d-9ce4-4419-8627-0f01b8672f38","_cell_guid":"a2840d11-a15e-4978-90e3-276b856ffd9d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-27T08:27:15.720169Z","iopub.execute_input":"2023-11-27T08:27:15.720558Z","iopub.status.idle":"2023-11-27T08:27:27.193219Z","shell.execute_reply.started":"2023-11-27T08:27:15.720529Z","shell.execute_reply":"2023-11-27T08:27:27.191989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"with import time= 11.26973295211792\nwithout import time=11.894261837005615 ","metadata":{}}]}